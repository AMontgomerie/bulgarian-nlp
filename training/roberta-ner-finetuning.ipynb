{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"roberta-ner-finetuning.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Djsvb7lg2tM8W-raTN__0ZHuW37Huo4j","authorship_tag":"ABX9TyNcFsWp8lnwTJtEjUhhDAxI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f94c5c49776e499ba0e8bea842f327d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0d383c614a0c462a99540ca0b0bf0e11","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f8288868ffb64c299b50f8cd08b079e4","IPY_MODEL_be1b8c265b8a4b41874c0c07eb9c308c"]}},"0d383c614a0c462a99540ca0b0bf0e11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f8288868ffb64c299b50f8cd08b079e4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b712953dd0e247ce8d93242199b3df1a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":515,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":515,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_885ea99a270042aba7398e355c619894"}},"be1b8c265b8a4b41874c0c07eb9c308c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7336740587b94f8db4fa65dbe2471267","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 515/515 [00:00&lt;00:00, 1.16kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2acadfb5e524448199ef87c67d5b0291"}},"b712953dd0e247ce8d93242199b3df1a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"885ea99a270042aba7398e355c619894":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7336740587b94f8db4fa65dbe2471267":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2acadfb5e524448199ef87c67d5b0291":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"89cad8b6d2cf43c982f03a89970a6d91":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3beeb2b2b0884723813f6db9bbf6b6a9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ada0a080eb044baba9aca7e524865942","IPY_MODEL_4eea56c59615475582750e75516d09eb"]}},"3beeb2b2b0884723813f6db9bbf6b6a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ada0a080eb044baba9aca7e524865942":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_546ee7fc76654474a2c1e65027608fa4","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":336426471,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":336426471,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3fb1c834abb04fb295a8b3381b3f87eb"}},"4eea56c59615475582750e75516d09eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f63a0e1eb8f44bd9b76ef707737d037b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 336M/336M [03:40&lt;00:00, 1.53MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_088a2d50cc52446781adc9f5065ba71d"}},"546ee7fc76654474a2c1e65027608fa4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3fb1c834abb04fb295a8b3381b3f87eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f63a0e1eb8f44bd9b76ef707737d037b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"088a2d50cc52446781adc9f5065ba71d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"nmo07OWYsXeo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":588},"executionInfo":{"status":"ok","timestamp":1598573654073,"user_tz":-540,"elapsed":7323,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}},"outputId":"a4490e3b-10ce-4035-917a-31474557696e"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\r\u001b[K     |▍                               | 10kB 30.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 4.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 5.1MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 5.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 4.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 5.5MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 6.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 7.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 6.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 778kB 6.0MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 20.1MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 27.9MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 29.4MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=770a86bfc5e394e46735cde1757fc6be50f448df3366769bd8031f26b04458f2\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W57vtTFJzuJb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598573657339,"user_tz":-540,"elapsed":8267,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}},"outputId":"85644a3b-16ee-4e27-95cb-689406c45dc1"},"source":["import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using device: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cF-nw1m5cnXx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1598573659503,"user_tz":-540,"elapsed":2124,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}},"outputId":"1ba3840c-9a1b-4d24-8895-a1573ecce91f"},"source":["!git clone https://github.com/usmiva/bg-ner"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'bg-ner'...\n","remote: Enumerating objects: 8, done.\u001b[K\n","remote: Counting objects: 100% (8/8), done.\u001b[K\n","remote: Compressing objects: 100% (8/8), done.\u001b[K\n","remote: Total 8 (delta 1), reused 4 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (8/8), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QvQl6nmFisvg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1598575842872,"user_tz":-540,"elapsed":22018,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}},"outputId":"a4016c5d-75e6-4785-9556-1c4cc7b9e49d"},"source":["from transformers import RobertaTokenizerFast\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import string\n","import re\n","\n","MODEL = \"iarfmoose/roberta-small-bulgarian\"\n","MAX_LEN = 128\n","\n","tokenizer = RobertaTokenizerFast.from_pretrained(MODEL, max_len=MAX_LEN)\n","\n","tag_to_id = {\n","    'O': 0,\n","    'I-PRO': 1,\n","    'I-PER': 2,\n","    'I-ORG': 3,\n","    'I-LOC': 4,\n","    'I-EVT': 5,\n","    'B-PRO': 6,\n","    'B-PER': 7,\n","    'B-ORG': 8,\n","    'B-LOC': 9,\n","    'B-EVT': 10\n","}\n","\n","id_to_tag = {tag_to_id[tag]: tag for tag in tag_to_id}\n","\n","class NERDataset(Dataset):\n","\n","    def __init__(self, filepath):\n","        sentences, ner_tags = self.parse_dataset(filepath)\n","\n","        error_count = 0\n","        self.data = []\n","        for row in zip(sentences, ner_tags):\n","            encoding = self.encode_sentence(row[0], row[1])\n","            if encoding:\n","                self.data.append(encoding)\n","            else:\n","                error_count += 1\n","        if error_count > 0:\n","            print('Was unable to encode {} examples'.format(error_count))\n","\n","    def __getitem__(self, index):\n","        item = self.data[index]\n","        item['input_ids'] = item['input_ids'].to(device)\n","        item['attention_mask'] = item['attention_mask'].to(device)\n","        item['labels'] = item['labels'].to(device)\n","        return item\n","    \n","    def __len__(self):\n","        return len(self.data)\n","\n","    def parse_dataset(self, filepath):\n","        with open(filepath, encoding='utf-8') as file:\n","            text = file.readlines()\n","\n","        text = [line.replace('\\n', '') for line in text]\n","        text = [line for line in text if len(line) > 0]\n","        word_list = [line.split('\\t')[0] for line in text]\n","        label_list = [line.split('\\t')[1] for line in text]\n","\n","        sentences = []\n","        tags = []\n","        current_sentence = []\n","        current_tags = []\n","        for item in zip(word_list, label_list):\n","            current_sentence.append(item[0])\n","            current_tags.append(item[1])\n","            if item[0] == '.':\n","                sentences.append(' '.join(current_sentence))\n","                tags.append(current_tags)\n","                current_sentence = []\n","                current_tags = []\n","        \n","        return sentences, tags\n","\n","    def encode_sentence(self, sentence, ner_tags):\n","        sentence = self.preprocess_punctuation(sentence)\n","        encoded_sentence = tokenizer(\n","            sentence, \n","            max_length=MAX_LEN,\n","            padding='max_length',\n","            truncation=True,\n","            add_special_tokens=True,\n","            return_offsets_mapping=True,\n","            return_tensors='pt'\n","        )\n","\n","        encoded_labels = self.encode_tags(ner_tags, encoded_sentence.offset_mapping)\n","\n","        if encoded_labels is not None:\n","            return {\n","                'input_ids': torch.squeeze(encoded_sentence.input_ids),\n","                'attention_mask': torch.squeeze(encoded_sentence.attention_mask),\n","                'labels': encoded_labels\n","            }\n","        else:\n","            return None\n","        \n","    def preprocess_punctuation(self, text):\n","        text = text.replace('©', '-')\n","        return text\n","\n","    # encodes labels in the last token position of each word\n","    def encode_tags(self, ner_tags, offset_mapping):\n","        labels = [tag_to_id[tag] for tag in ner_tags]\n","        encoded_labels = np.ones(len(offset_mapping), dtype=int) * -100\n","\n","        for i in range(1, len(offset_mapping) - 1):\n","            \n","            if offset_mapping[i][1] != offset_mapping[i+1][0]:\n","                if not self.ignore_mapping(offset_mapping[i]):\n","                    try:\n","                        encoded_labels[i] = labels.pop(0)\n","                    except(IndexError):\n","                        return None\n","        \n","        if len(labels) > 0:\n","            return None\n","\n","        return torch.tensor(encoded_labels)\n","\n","    def ignore_mapping(self, mapping):\n","        return mapping[0] == mapping[1]\n","\n","train_set = NERDataset('bg-ner/train.txt')\n","test_set = NERDataset('bg-ner/test.txt')\n","train_loader = DataLoader(train_set, shuffle=True, batch_size=16)\n","test_loader = DataLoader(test_set, shuffle=False, batch_size=16)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Was unable to encode 24 examples\n","Was unable to encode 3 examples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TzH53UBU6Ok9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f94c5c49776e499ba0e8bea842f327d5","0d383c614a0c462a99540ca0b0bf0e11","f8288868ffb64c299b50f8cd08b079e4","be1b8c265b8a4b41874c0c07eb9c308c","b712953dd0e247ce8d93242199b3df1a","885ea99a270042aba7398e355c619894","7336740587b94f8db4fa65dbe2471267","2acadfb5e524448199ef87c67d5b0291","89cad8b6d2cf43c982f03a89970a6d91","3beeb2b2b0884723813f6db9bbf6b6a9","ada0a080eb044baba9aca7e524865942","4eea56c59615475582750e75516d09eb","546ee7fc76654474a2c1e65027608fa4","3fb1c834abb04fb295a8b3381b3f87eb","f63a0e1eb8f44bd9b76ef707737d037b","088a2d50cc52446781adc9f5065ba71d"]},"executionInfo":{"status":"ok","timestamp":1598575872568,"user_tz":-540,"elapsed":24364,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}},"outputId":"a51e6731-c2e6-4503-90d0-98c544331251"},"source":["from transformers import RobertaForTokenClassification\n","\n","learning_rate = 1e-5\n","\n","model = RobertaForTokenClassification.from_pretrained(\n","    MODEL, \n","    num_labels=len(tag_to_id)\n",")\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f94c5c49776e499ba0e8bea842f327d5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=515.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89cad8b6d2cf43c982f03a89970a6d91","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=336426471.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at iarfmoose/roberta-small-bulgarian were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at iarfmoose/roberta-small-bulgarian and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["RobertaForTokenClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"WsBh_YHo6lJW","colab_type":"code","colab":{}},"source":["LOG_INTERVAL = round(len(train_loader) / 10)\n","\n","def train(epoch):\n","    model.train()\n","    total_loss = 0\n","\n","    for batch_index, batch in enumerate(train_loader):\n","        model.zero_grad()\n","        output = model(**batch)\n","        loss = output[0]\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        if batch_index % LOG_INTERVAL == 0 and batch_index > 0:\n","            current_loss = total_loss / LOG_INTERVAL\n","            print('| epoch {:3d} | ' \n","                  '{:5d}/{:5d} batches | '\n","                  'loss {:5.2f}'.format(\n","                    epoch, \n","                    batch_index, len(train_loader), \n","                    current_loss))\n","            total_loss = 0\n","\n","def test(data_loader):\n","    model.eval()\n","    total_score = 0\n","    total_len = 0\n","\n","    with torch.no_grad():\n","        for batch_index, batch in enumerate(data_loader):\n","            output = model(**batch)\n","            preds = np.argmax(output[1].cpu(), axis=2)\n","            preds = preds[(batch['labels'] != -100)]\n","            labels = batch['labels'][(batch['labels'] != -100)]\n","            total_score += preds.eq(labels.cpu()).sum()\n","            total_len += len(labels)\n","    return (total_score.item() / total_len) * 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aZ_5Q1ee6qRv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":420},"executionInfo":{"status":"ok","timestamp":1598575985499,"user_tz":-540,"elapsed":129819,"user":{"displayName":"Adam Montgomerie","photoUrl":"","userId":"08195438125971362273"}},"outputId":"d8903b84-7636-4a1c-a288-badd36f59e2c"},"source":["EPOCHS = 2\n","\n","accuracy = test(test_loader)\n","print('| Pretraining Accuracy: {:.2f}%\\n'.format(accuracy))\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train(epoch)\n","    accuracy = test(test_loader)\n","    print('| epoch   {} |  Accuracy: {:.2f}%\\n'.format(epoch, accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["| Pretraining Accuracy: 9.71%\n","\n","| epoch   1 |    45/  450 batches | loss  0.75\n","| epoch   1 |    90/  450 batches | loss  0.29\n","| epoch   1 |   135/  450 batches | loss  0.21\n","| epoch   1 |   180/  450 batches | loss  0.15\n","| epoch   1 |   225/  450 batches | loss  0.12\n","| epoch   1 |   270/  450 batches | loss  0.13\n","| epoch   1 |   315/  450 batches | loss  0.11\n","| epoch   1 |   360/  450 batches | loss  0.10\n","| epoch   1 |   405/  450 batches | loss  0.08\n","| epoch   1 |  Accuracy: 96.86%\n","\n","| epoch   2 |    45/  450 batches | loss  0.07\n","| epoch   2 |    90/  450 batches | loss  0.06\n","| epoch   2 |   135/  450 batches | loss  0.06\n","| epoch   2 |   180/  450 batches | loss  0.06\n","| epoch   2 |   225/  450 batches | loss  0.05\n","| epoch   2 |   270/  450 batches | loss  0.06\n","| epoch   2 |   315/  450 batches | loss  0.06\n","| epoch   2 |   360/  450 batches | loss  0.05\n","| epoch   2 |   405/  450 batches | loss  0.05\n","| epoch   2 |  Accuracy: 97.88%\n","\n"],"name":"stdout"}]}]}